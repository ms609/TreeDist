<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Comparing splits using information theory • TreeDist</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.3/docsearch.min.css" integrity="sha256-QOSRU/ra9ActyXkIBbiIB144aDBdtvXBcNc3OTNuX/Q=" crossorigin="anonymous">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="Comparing splits using information theory">
<meta property="og:description" content="TreeDist">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-156666971-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-156666971-1');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">TreeDist</a>
        <span class="version label label-danger" data-toggle="tooltip" data-placement="bottom" title="Unreleased version">0.0.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/Using-TreeDist.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Using-TreeDist.html">Using TreeDist to calculate tree similarity</a>
    </li>
    <li>
      <a href="../articles/using-distances.html">Contextualizing tree distances</a>
    </li>
    <li>
      <a href="../articles/Robinson-Foulds.html">Robinson-Foulds metric</a>
    </li>
    <li>
      <a href="../articles/Generalized-RF.html">Generalized Robinson-Foulds distances</a>
    </li>
    <li>
      <a href="../articles/information.html">Concepts of information</a>
    </li>
    <li>
      <a href="../articles/Tree-distance-metric-evaluation.html">Evaluating tree distance metrics</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/ms609/TreeDist">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right hidden-xs hidden-sm" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Comparing splits using information theory</h1>
                        <h4 class="author">Martin R. Smith</h4>
            
            <h4 class="date">2020-06-18</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/ms609/TreeDist/blob/master/vignettes/information.Rmd"><code>vignettes/information.Rmd</code></a></small>
      <div class="hidden name"><code>information.Rmd</code></div>

    </div>

    
    
<p>To understand the <a href="Generalized-RF.html">information-based metrics</a> implemented in <a href="Using-TreeDist.html"><code>TreeDist</code></a>, it is useful to recall some basic concepts of information theory (see <span class="citation">MacKay (2003)</span> for an introduction).</p>
<div id="shannon-information-content" class="section level2">
<h2 class="hasAnchor">
<a href="#shannon-information-content" class="anchor"></a>Shannon information content</h2>
<p>Information is usually measured in <em>bits</em>. One bit is the amount of information generated by tossing a fair coin: to record the outcome of a coin toss, I must record either a <code>H</code> or a <code>T</code>, and with each of the two symbols equally likely, there is no way to compress the results of multiple tosses.</p>
<p>The Shannon information content of an outcome <span class="math inline">\(x\)</span> is defined to be <span class="math inline">\(h(x) = -\log_2{P(x)}\)</span>, which simplifies to <span class="math inline">\(\log_2{n}\)</span> when all <span class="math inline">\(n\)</span> outcomes are equiprobable. Thus, the information content of a fair coin toss is <span class="math inline">\(\log_2{2} = 1\textrm{ bit}\)</span>; the information content of rolling a six-sided die is <span class="math inline">\(\log_2{6} \approx 2.58\textrm{ bits}\)</span>; and the information content of selecting at random one of the 105 unrooted binary six-leaf trees is <span class="math inline">\(\log_2{105} \approx 6.71\textrm{ bits}\)</span>.</p>
<div id="application-to-splits" class="section level3">
<h3 class="hasAnchor">
<a href="#application-to-splits" class="anchor"></a>Application to splits</h3>
<p>The split <span class="math inline">\(S_1 =\)</span> <code>AB|CDEF</code> is found in 15 of the 105 six-leaf trees; as such, the probability that a randomly drawn tree contains <span class="math inline">\(S_1\)</span> is <span class="math inline">\(P(S_1) = \frac{15}{105}\)</span>, and the information content <span class="math inline">\(h(S_1) = -\log_2{\frac{15}{105}} \approx 2.81\textrm{ bits}\)</span>. <span class="citation">Steel &amp; Penny (2006)</span> dub this quantity the phylogenetic information content.</p>
<p>Likewise, the split <span class="math inline">\(S_2 =\)</span> <code>ABC|DEF</code> occurs in nine six-leaf trees, so <span class="math inline">\(h(S_2) = -\log_2{\frac{9}{105}} \approx 3.54\textrm{ bits}\)</span>. Three six-leaf trees contain both splits, so in combination the splits deliver <span class="math inline">\(h(S_1,S_2) = -\log_2{\frac{3}{105}} \approx 5.13\textrm{ bits}\)</span> of information.</p>
<p>Because <span class="math inline">\(h(S_1,S_2) &lt; h(S_1) + h(S_2)\)</span>, some of the information in <span class="math inline">\(S_1\)</span> is also present in <span class="math inline">\(S_2\)</span>. The information in common between <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> is <span class="math inline">\(h_{shared}(S_1, S_2) = h(S_1) + h(S_2) - h(S_1,S_2) \approx 1.22\textrm{ bits}\)</span>. The information unique to <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> is <span class="math inline">\(h_{different}(S_1,S_2) = 2h(S_1,S_2) - h(S_1) - h(S_2) \approx 3.91\textrm{ bits}\)</span>.</p>
<p>These quantities can be calculated using functions in the ‘<a href="https://ms609.github.io/TreeTools">TreeTools</a>’ package.</p>
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="st">'TreeTools'</span>, <span class="kw">quietly</span> <span class="kw">=</span> <span class="fl">TRUE</span>, <span class="kw">warn.conflicts</span> <span class="kw">=</span> <span class="fl">FALSE</span>)
<span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="st">'TreeDist'</span>)
<span class="no">treesMatchingSplit</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(
  <span class="kw">AB.CDEF</span> <span class="kw">=</span> <span class="fu"><a href="https://ms609.github.io/TreeTools/reference/TreesMatchingSplit.html">TreesMatchingSplit</a></span>(<span class="fl">2</span>, <span class="fl">4</span>),
  <span class="kw">ABC.DEF</span> <span class="kw">=</span> <span class="fu"><a href="https://ms609.github.io/TreeTools/reference/TreesMatchingSplit.html">TreesMatchingSplit</a></span>(<span class="fl">3</span>, <span class="fl">3</span>)
)
<span class="no">treesMatchingSplit</span></pre></body></html></div>
<pre><code>## AB.CDEF ABC.DEF 
##      15       9</code></pre>
<div class="sourceCode" id="cb3"><html><body><pre class="r"><span class="no">proportionMatchingSplit</span> <span class="kw">&lt;-</span> <span class="no">treesMatchingSplit</span> / <span class="fu"><a href="https://ms609.github.io/TreeTools/reference/NRooted.html">NUnrooted</a></span>(<span class="fl">6</span>)
<span class="no">proportionMatchingSplit</span></pre></body></html></div>
<pre><code>##    AB.CDEF    ABC.DEF 
## 0.14285714 0.08571429</code></pre>
<div class="sourceCode" id="cb5"><html><body><pre class="r"><span class="no">splitInformation</span> <span class="kw">&lt;-</span> -<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span>(<span class="no">proportionMatchingSplit</span>)
<span class="no">splitInformation</span></pre></body></html></div>
<pre><code>##  AB.CDEF  ABC.DEF 
## 2.807355 3.544321</code></pre>
<div class="sourceCode" id="cb7"><html><body><pre class="r"><span class="no">treesMatchingBoth</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/SplitSharedInformation.html">TreesConsistentWithTwoSplits</a></span>(<span class="fl">6</span>, <span class="fl">2</span>, <span class="fl">3</span>)
<span class="no">combinedInformation</span> <span class="kw">&lt;-</span> -<span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span>(<span class="no">treesMatchingBoth</span> / <span class="fu"><a href="https://ms609.github.io/TreeTools/reference/NRooted.html">NUnrooted</a></span>(<span class="fl">6</span>))

<span class="no">sharedInformation</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span>(<span class="no">splitInformation</span>) - <span class="no">combinedInformation</span>
<span class="no">sharedInformation</span></pre></body></html></div>
<pre><code>## [1] 1.222392</code></pre>
<div class="sourceCode" id="cb9"><html><body><pre class="r"><span class="co"># Or more concisely:</span>
<span class="fu"><a href="../reference/SplitSharedInformation.html">SplitSharedInformation</a></span>(<span class="kw">n</span> <span class="kw">=</span> <span class="fl">6</span>, <span class="fl">2</span>, <span class="fl">3</span>)</pre></body></html></div>
<pre><code>## [1] 1.222392</code></pre>
<!--The more similar the splits, the more information they will have in common;
the shared information is maximised when $S_1 = S_2$, and $h_{common} = h(S_1)$.
Likewise, more even splits contain more information than less even splits
(i.e. _h_(`AB|CDEF`) < _h_(`ABC|DEF`)).-->
</div>
</div>
<div id="entropy" class="section level2">
<h2 class="hasAnchor">
<a href="#entropy" class="anchor"></a>Entropy</h2>
<p>Entropy is the average information content of each outcome, weighted by its probability: <span class="math inline">\(\sum{-p \log_2(p)}\)</span>. Where all <em>n</em> outcomes are equiprobable, this simplifies to <span class="math inline">\(\log_2{n}\)</span>.</p>
<p>Consider a case in which Jane rolls a dice, and makes two true statements about the outcome <span class="math inline">\(x\)</span>:</p>
<p><span class="math inline">\(S_1\)</span>: “Is the roll even?”.</p>
<ul>
<li>Two equally-possible outcomes: yes or no</li>
<li>Entropy: <span class="math inline">\(H(S_1) = \log_2{2} = 1\textrm{ bit}\)</span>.</li>
</ul>
<p><span class="math inline">\(S_2\)</span>: “Is the roll greater than 3?”</p>
<ul>
<li>Two equally-possible outcomes: yes or no</li>
<li>Entropy: <span class="math inline">\(H(S_2) = \log_2{2} = 1\textrm{ bit}\)</span>.</li>
</ul>
<p>The joint entropy of <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> is the entropy of the association matrix that considers each possible outcome:</p>
<table class="table">
<thead><tr class="header">
<th> </th>
<th>
<span class="math inline">\(S_1: x\)</span> odd</th>
<th>
<span class="math inline">\(S_1: x\)</span> even</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(S_2: x \le 3\)</span></td>
<td><span class="math inline">\(x \in {1, 3}; p = \frac{2}{6}\)</span></td>
<td><span class="math inline">\(x = 2; p = \frac{1}{6}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(S_2: x &gt; 3\)</span></td>
<td><span class="math inline">\(x = 5; p = \frac{1}{6}\)</span></td>
<td><span class="math inline">\(x \in {4, 6}; p = \frac{2}{6}\)</span></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\begin{aligned} H(S_1, S_2) = \frac{2}{3}\log_2{\frac{2}{3}} + \frac{1}{3}\log_2{\frac{1}{3}} + \frac{1}{3}\log_2{\frac{1}{3}} + \frac{2}{3}\log_2{\frac{2}{3}} \approx 1.84 \textrm{ bits} \end{aligned}\)</span></p>
<p>Note that this less than the <span class="math inline">\(\log_2{6} \approx 2.58\textrm{ bits}\)</span> we require to determine the exact value of the roll: knowledge of <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> is not guaranteed to be sufficient to unambiguously identify <span class="math inline">\(x\)</span>.</p>
<p>The mutual information between <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> describes how much knowledge of <span class="math inline">\(S_1\)</span> reduces our uncertainty in <span class="math inline">\(S_2\)</span> (or <em>vice versa</em>). So if we learn that <span class="math inline">\(S_1\)</span> is ‘even’, we become a little more confident that <span class="math inline">\(S_2\)</span> is ‘greater than three’.</p>
<p>The mutual information <span class="math inline">\(I(S_1;S_2)\)</span> corresponds to the sum of the individual entropies, minus the joint entropy:</p>
<p>\begin{aligned} I(S_1;S_2) = H(S_1) + H(S_2) - H(S_1, S_2) \end{aligned}</p>
<p>The entropy distance, also termed the variation of information <span class="citation">(Meilă, 2007)</span>, is the information that <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> do <em>not</em> have in common:</p>
<p>\begin{aligned} H_D(S_1, S_2) = H(S_1, S_2) - I(S_1;S_2) = 2H(S_1, S_2) - H(S_1) - H(S_2) \end{aligned}</p>
<p><img src="information_files/figure-html/mackay-8-1-1.png" width="50%" style="display: block; margin: auto;"></p>
<div id="application-to-splits-1" class="section level3">
<h3 class="hasAnchor">
<a href="#application-to-splits-1" class="anchor"></a>Application to splits</h3>
<p>Let split <span class="math inline">\(S\)</span> divides <span class="math inline">\(n\)</span> leaves into two partitions <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. The probability that a randomly chosen leaf <span class="math inline">\(x\)</span> is in partition <span class="math inline">\(k\)</span> is <span class="math inline">\(P(x \in k) = \frac{|k|}{n}\)</span>. <span class="math inline">\(S\)</span> thus corresponds to a random variable with entropy <span class="math inline">\(H(S) = -\frac{|A|}{n} \log_2{\frac{|A|}{n}} - \frac{|B|}{n}\log_2{\frac{|B|}{n}}\)</span> <span class="citation">(Meilă, 2007)</span>.</p>
<p>The entropy of a split corresponds to the average number of bits necessary to transmit the partition label for a given leaf.</p>
<p>The joint entropy of two splits, <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>, corresponds to the entropy of the association matrix of probabilities that a randomly selected leaf belongs to each pair of partitions:</p>
<table class="table">
<colgroup>
<col width="23%">
<col width="44%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th> </th>
<th><span class="math inline">\(S_1: x \in A_1\)</span></th>
<th><span class="math inline">\(S_1: x \in B_1\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(S_2: x \in A_2\)</span></td>
<td><span class="math inline">\(P(A_1,A_2) = \frac{|A_1 \cap A_2|}{n}\)</span></td>
<td><span class="math inline">\(P(B_1,A_2) = \frac{|B_1 \cap A_2|}{n}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(S_2: x \in B_2\)</span></td>
<td><span class="math inline">\(P(A_1,B_2) = \frac{|A_1 \cap B_2|}{n}\)</span></td>
<td><span class="math inline">\(P(B_1,B_2) = \frac{|B_1 \cap B_2|}{n}\)</span></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(H(S_1, S_2) = P(A_1,A_2) \log_2 {P(A_1,A_2)} + P(B_1,A_2) \log_2 {P(B_1,A_2)}\)</span></p>
<p><span class="math inline">\(+ P(A_1,B_2)\log_2{P(A_1,B_2)} + P(B_1,B_2)\log_2{P(B_1,B_2)}\)</span></p>
<p>These values can then be substituted into the definitions of mutual information and entropy distance given above.</p>
<p>As <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> become more different, the disposition of <span class="math inline">\(S_1\)</span> gives less information about the configuration of <span class="math inline">\(S_2\)</span>, and the mutual information decreases accordingly.</p>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-Mackay2003">
<p>MacKay, D. J. C. (2003). <em>Information theory, inference, and learning algorithms</em>. Cambridge: Cambridge University Press. Retrieved from <a href="https://www.inference.org.uk/itprnn/book.pdf" class="uri">https://www.inference.org.uk/itprnn/book.pdf</a></p>
</div>
<div id="ref-Meila2007">
<p>Meilă, M. (2007). Comparing clusterings—an information based distance. <em>Journal of Multivariate Analysis</em>, <em>98</em>(5), 873–895. doi:<a href="https://doi.org/10.1016/j.jmva.2006.11.013">10.1016/j.jmva.2006.11.013</a></p>
</div>
<div id="ref-Steel2006">
<p>Steel, M. A., &amp; Penny, D. (2006). Maximum parsimony and the phylogenetic information in multistate characters. In V. A. Albert (Ed.), <em>Parsimony, phylogeny, and genomics</em> (pp. 163–178). Oxford: Oxford University Press.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by <a href="https://community.dur.ac.uk/martin.smith/">Martin R. Smith</a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.js" integrity="sha256-GKvGqXDznoRYHCwKXGnuchvKSwmx9SRMrZOTh2g4Sb0=" crossorigin="anonymous"></script><script>
  docsearch({
    
    
    apiKey: '8bf44d312479af73720c5ca5fcc12eba',
    indexName: 'treedist',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
