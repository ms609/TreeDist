% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hierarchical_mutual_information.R
\name{HierarchicalMutualInfo}
\alias{HierarchicalMutualInfo}
\alias{HMI}
\alias{SelfHMI}
\alias{EHMI}
\alias{AHMI}
\title{Hierarchical Mutual Information for phylogenetic trees}
\usage{
HierarchicalMutualInfo(tree1, tree2 = NULL, normalize = FALSE)

HMI(tree1, tree2 = NULL, normalize = FALSE)

SelfHMI(tree)

EHMI(tree1, tree2, tolerance = 0.01, minResample = 36)

AHMI(tree1, tree2, Mean = max, tolerance = 0.01, minResample = 36)
}
\arguments{
\item{tree1, tree2}{Trees of class \code{phylo}, or lists of such trees.
If \code{tree2} is not provided, distances will be calculated between
each pair of trees in the list \code{tree1}.}

\item{normalize}{If \code{FALSE}, return the raw \acronym{HMI}, in bits.
If \code{TRUE}, normalize to range [0,1] by dividing by
\code{max(SelfHMI(tree1), SelfHMI(tree2))}.
If a function, divide by \code{normalize(SelfHMI(tree1), SelfHMI(tree2))}.}
}
\value{
\code{HierarchicalMutualInfo()} returns a numeric value representing the
Hierarchical Mutual Information between the input trees, in bits,
normalized as specified.
Higher values indicate more shared hierarchical structure.

\code{SelfHMI()} returns the hierarchical mutual information of a tree
compared with itself, i.e. its hierarchical entropy (\acronym{HH}).

\code{EHMI()} returns the expected \acronym{HMI} against a uniform
shuffling of element labels, estimated by performing Monte Carlo resampling
on the same hierarchical structure until the standard error of the
estimate falls below \code{tolerance}.
The attributes of the returned object list the variance (\code{var}),
standard deviation (\code{sd}), standard error of the mean (\code{sem}) and
relative error (\code{relativeError}) of the estimate, and the number of Monte
Carlo samples used to obtain it (\code{samples}).

\code{AHMI()} returns the adjusted \acronym{HMI}, normalized such that
zero corresponds to the expected \acronym{HMI} given a random shuffling
of elements on the same hierarchical structure.  The attribute \code{sem} gives
the standard error of the estimate.
}
\description{
Calculate the Hierarchical Mutual Information (\acronym{HMI})
between two phylogenetic trees, following the recursive algorithm of
\insertCite{Perotti2020;textual}{TreeDist}.
}
\details{
\code{HierarchicalMutualInfo()} computes the hierarchical mutual content of trees
\insertCite{Perotti2015,Perotti2020}{TreeDist}, which accounts for the
non-independence of information represented by nested splits.

\code{tree} is converted to a set of hierarchical partitions, and the mutual
information (in bits) is computed recursively; the contribution of a node is
given by:

\deqn{I(t,s) = \log_2(n_{ts}) - \dfrac{H_{us} + H_{tv} - H_{uv}}{n_{ts}} +
\text{mean}(I_{uv})}

Where:
\itemize{
\item \eqn{n_{ts}} is the number of common elements between partitions
\item \eqn{H_{us}, H_{tv}, H_{uv}} are entropy terms from child comparisons
\item \eqn{I_{uv}} is the recursive \acronym{HMI} for child pairs
}
}
\examples{
library("TreeTools", quietly = TRUE)

tree1 <- BalancedTree(8)
tree2 <- PectinateTree(8)

# Calculate HMI between two trees
HierarchicalMutualInfo(tree1, tree2)

# HMI normalized against the mean information content of tree1 and tree2
HierarchicalMutualInfo(tree1, tree2, normalize = mean)

# Normalized HMI above is equivalent to:
HMI(tree1, tree2) / mean(SelfHMI(tree1), SelfHMI(tree2))
# Expected mutual info for this pair of hierarchies
EHMI(tree1, tree2, tolerance = 0.1)
# The adjusted HMI normalizes against this expectation
AHMI(tree1, tree2, tolerance = 0.1)
}
\references{
\insertAllCited{}
}
\seealso{
Other tree distances: 
\code{\link{JaccardRobinsonFoulds}()},
\code{\link{KendallColijn}()},
\code{\link{MASTSize}()},
\code{\link{MatchingSplitDistance}()},
\code{\link{NNIDist}()},
\code{\link{NyeSimilarity}()},
\code{\link{PathDist}()},
\code{\link{Robinson-Foulds}},
\code{\link{SPRDist}()},
\code{\link{TreeDistance}()}
}
\concept{tree distances}
