% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hierarchical_mutual_information.R
\name{HierarchicalMutualInfo}
\alias{HierarchicalMutualInfo}
\alias{HMI}
\alias{HierarchicalMutualInformation}
\alias{SelfHMI}
\alias{EHMI}
\alias{AHMI}
\title{Hierarchical Mutual Information for phylogenetic trees}
\usage{
HierarchicalMutualInfo(tree1, tree2 = NULL, normalize = FALSE)

HMI(tree1, tree2 = NULL, normalize = FALSE)

HierarchicalMutualInformation(tree1, tree2 = NULL, normalize = FALSE)

SelfHMI(tree)

EHMI(tree1, tree2, tolerance = 0.01, minResample = 36)

AHMI(tree1, tree2, Mean = max, tolerance = 0.01, minResample = 36)
}
\arguments{
\item{tree1, tree2}{Trees of class \code{phylo}, or lists of such trees.
If \code{tree2} is not provided, distances will be calculated between
each pair of trees in the list \code{tree1}.}

\item{normalize}{If \code{FALSE}, do not normalize the result.  If a function,
Normalize the result to range [0,1] by dividing by
\code{Func(SelfHMI(tree1), SelfHMI(tree2))}, where \code{Func()} = \code{max()} if
\code{normalize == TRUE}, \code{normalize()} otherwise.}
}
\value{
A numeric value representing the Hierarchical Mutual Information
between the input trees. Higher values indicate more shared
hierarchical structure.

\code{SelfHMI()} returns the hierarchical mutual information of a tree
compared with itself, i.e. its hierarchical entropy (\acronym{HH}).

\code{EHMI()} returns the expected \acronym{HMI} against a uniform
shuffling of element labels, estimated by performing Monte Carlo resampling
on the same hierarchical structure until the standard error of the
estimate falls below \code{tolerance}.
The attributes of the returned object list the variance (\code{var}),
standard deviation (\code{sd}), standard error of the mean (\code{sem}) and
relative error (\code{relativeError}) of the estimate, and the number of Monte
Carlo samples used to obtain it (\code{samples}).

\code{AHMI()} returns the adjusted \acronym{HMI}, normalized such that
zero corresponds to the expected \acronym{HMI} given a random shuffling
of elements on the same hierarchical structure.  The attribute \code{sem} gives
the standard error of the estimate.
}
\description{
Calculate the Hierarchical Mutual Information (\acronym{HMI})
between two phylogenetic trees, following the recursive algorithm from
\insertCite{Perotti2015,Perotti2020;textual}{TreeDist}.
}
\details{
This function implements the recursive Hierarchical Mutual Information algorithm
that considers the nested, hierarchical structure of phylogenetic trees when
computing information measures. The algorithm converts trees to hierarchical
partitions and computes mutual information recursively using natural logarithm.

The recursive \acronym{HMI} formula for internal nodes is:
\deqn{I(t,s) = ln(n_ts) - (H_us + H_tv - H_uv)/n_ts + mean(I_uv)}

Where:
\itemize{
\item \eqn{n_ts} is the number of common elements between partitions
\item \eqn{H_us, H_tv, H_uv} are entropy terms from child comparisons
\item \eqn{I_uv} is the recursive \acronym{HMI} for child pairs
}
}
\examples{
library("TreeTools", quietly = TRUE)

tree1 <- BalancedTree(8)
tree2 <- PectinateTree(8)

# Calculate HMI between two trees
HierarchicalMutualInfo(tree1, tree2)

# HMI normalized against the mean information content of tree1 and tree2
HierarchicalMutualInfo(tree1, tree2, normalize = mean)

# Normalized HMI above is equivalent to:
HMI(tree1, tree2) / mean(SelfHMI(tree1), SelfHMI(tree2))
# Expected mutual info for this pair of hierarchies
EHMI(tree1, tree2, tolerance = 0.1)
# The adjusted HMI normalizes against this expectation
AHMI(tree1, tree2, tolerance = 0.1)
}
\references{
\insertAllCited{}
}
\seealso{
Other tree distances: 
\code{\link{JaccardRobinsonFoulds}()},
\code{\link{KendallColijn}()},
\code{\link{MASTSize}()},
\code{\link{MatchingSplitDistance}()},
\code{\link{NNIDist}()},
\code{\link{NyeSimilarity}()},
\code{\link{PathDist}()},
\code{\link{Robinson-Foulds}},
\code{\link{SPRDist}()},
\code{\link{TreeDistance}()}
}
\concept{tree distances}
