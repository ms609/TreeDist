% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hierarchical_mutual_information.R
\name{CharH}
\alias{CharH}
\alias{CharJH}
\alias{CharMI}
\alias{CharEJH}
\alias{CharEMI}
\alias{CharAMI}
\title{Mutual information between a character and a tree}
\usage{
CharH(tree)

CharJH(char, tree)

CharMI(char, tree)

CharEJH(char, tree, precision = 0.01, minSample = 36, nCores = 1)

CharEMI(char, tree, precision = 0.01, minSample = 36, nCores = 2)

CharAMI(
  char,
  tree,
  Mean = function(charH, treeH) charH,
  precision = 0.01,
  minSample = 36,
  nCores = 2
)
}
\arguments{
\item{tree}{Hierarchical structure, such as a phylogenetic tree,
that can be coerced to an \code{\link{HPart}} object.}

\item{char}{Vector that labels each leaf in \code{tree} such that entries with
the same label share a common character state.}

\item{precision}{Numeric; Monte Carlo sampling will terminate once the
relative (or for \code{CharAMI()}, absolute) standard error falls below this
value.}

\item{minSample}{Integer specifying minimum number of Monte Carlo samples
to conduct per chain.
Avoids early termination when sample size is too small to reliably estimate
the standard error of the mean.}

\item{nCores}{Integer specifying number of cores to employ in calculation.
Performance gains may only be evident in large trees.}

\item{Mean}{Function by which to combine the self-information of the
two input hierarchies, in order to normalize the \acronym{HMI}.}
}
\value{
\code{CharH()} returns the entropy of a tree, in bits, defined as its
capacity to assign leaves to unique clusters.
Because leaves in a cherry are not classed as distinguished, the entropy of
a tree is always less than \eqn{N \log2(N)}.

\code{CharJH()} returns the joint entropy of \code{char} and \code{tree}, in bits,
defined as the combined capacity to assign leaves to unique clusters.

\code{CharMI()} returns the mutual information content of \code{char} and
\code{tree}, in bits.

\code{CharEJH()} returns the expected joint entropy of \code{char} and \code{tree},
in bits, where state labels are shuffled at random.
Estimates based on the joint entropy bear the attributes:
\itemize{
\item \code{"ejh"}: Estimate of the expected joint entropy
\item \code{"ejhVar"}, \code{"ejhSD"}, \code{"ejhSEM"}: variance, standard deviation and
absolute standard error of the mean of the estimated joint entropy
\item \code{"sem"}: Standard error of the mean of the returned value
\item \code{"precision"}: Precision achieved, in relative terms (\code{CharEJH()},
\code{CharEMI()}) or absolute terms (\code{CharAMI()}; or other functions where
the returned value is close to zero).
}

\code{CharEMI()} returns the expected mutual information of \code{char} and
\code{tree}, in bits, when state labels are shuffled at random.

\code{CharAMI()} returns the expected mutual information of \code{char} and
\code{tree}, in bits, when state labels are shuffled at random.
The "precision" attribute reports the absolute error of the estimate
}
\description{
Mutual information between a character and a tree
}
\author{
\href{https://orcid.org/0000-0001-5660-1727}{Martin R. Smith}
(\href{mailto:martin.smith@durham.ac.uk}{martin.smith@durham.ac.uk})
}
