---
title: "Information theory"
author: "Martin R. Smith"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
csl: ../inst/apa-old-doi-prefix.csl
vignette: >
  %\VignetteIndexEntry{Concepts of information}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

To understand the [information-based metrics](Generalized-RF.html) implemented
in [`TreeDist`](Using-TreeDist.html), it is useful to recall some basic concepts
of information theory.  The examples below ape @Mackay2003.

## Shannon information content

Information is usually measured in _bits_.  One bit is the amount of information
generated by tossing a fair coin: to record the outcome of a coin toss, I must
record either a `H` or a `T`, and with each of the two symbols equally likely,
there is no way to compress the results of multiple tosses.

The Shannon information content of an outcome $x$ is defined to be $h(x) = 
-\log_2{P(x)}$, which simplifies to $\log_2(n)$ when all $n$ outcomes are
equiprobable.  Thus, the information content of a fair coin toss is
$\log_2{2} = 1\textrm{ bit}$; the information content of rolling a six-sided
die is $\log_2{6} \approx 2.58\textrm{ bits}$; and the information content of 
selecting at random one of the 105 unrooted binary six-leaf trees is 
$\log_2{105} \approx 6.71\textrm{ bits}$.

Now, the split $S_1 =$ `AB|CDEF` is found in 15 of the 105 six-leaf trees; 
as such, the probability that a randomly drawn tree contains $S_1$ is 
$P(S_1) = \frac{15}{105}$, and the information content $h(S_1) = 
-\log_2{\frac{15}{105}} \approx 2.81\textrm{ bits}$. @Steel2006 dub this 
quantity the phylogenetic information content.

Likewise, the split $S_2 =$ `ABC|DEF` occurs in nine six-leaf trees, so 
$h(S_2) = -\log_2{\frac{9}{105}} \approx 3.54\textrm{ bits}$. Three six-leaf 
trees contain both splits, so $h(S_1,S_2) = -\log_2{\frac{3}{105}} \approx 
5.13\textrm{ bits}$.

Because $h(S_1,S_2) < h(S_1) + h(S_2)$, some of the information in $S_1$ is
duplicated in $S_2$.
The information in common between $S_1$ and $S_2$ is $h_{common} = h(S_1) + 
h(S_2) - h(S_1,S_2) \approx 1.22\textrm{ bits}$.

<!--The more similar the splits, the more information they will have in common;
the shared information is maximised when $S_1 = S_2$, and $h_{common} = h(S_1)$.
Likewise, more even splits contain more information than less even splits
(i.e. _h_(`AB|CDEF`) < _h_(`ABC|DEF`)).-->

## Entropy

Entropy is the average information content of each outcome, weighted by its
probability: $\sum{-p \log_2(p)}$.  Where all _n_ outcomes are equiprobable, 
this simplifies to $\log_2{n}$.

Consider a case in which Jane rolls a dice, and offers to truthfully answer
two questions:

A: "Is the roll even?".

- Two equally-possible outcomes: yes or no
- Entropy: $H(A) = \log_2{2} = 1\textrm{ bit}$.

B: "Is the roll greater than 3?"

- Two equally-possible outcomes: yes or no
- Entropy: $H(B) = \log_2{2} = 1\textrm{ bit}$.


The joint entropy of A and B is the entropy of the association 
matrix that considers each possible outcome:

 &nbsp;        | A: odd                   | A: even
-------------- | ------------------------ | -----
**B: &le; 3**  | $p = \frac{2}{6}$ (1, 3) | $p = \frac{1}{6}$ (2) |
**B: > 3**     | $p = \frac{1}{6}$ (5)    | $p = \frac{2}{6}$ (4, 6) |

$\begin{aligned}
H(A, B) = \frac{2}{3}\log_2\frac{2}{3} + \frac{1}{3}\log_2\frac{1}{3} + \frac{1}{3}\log_2\frac{1}{3} + \frac{2}{3}\log_2\frac{2}{3} = 1.84 \textrm{ bits}
\end{aligned}$
 

Note that this less than the 2.58 bits we require to determine the exact value
of the roll: knowledge of _A_ and _B_ is not guaranteed to be sufficient to
unambiguously identify the outcome.


The mutual information between _A_ and _B_ describes how much knowledge of _A_ 
reduces our uncertainty in _B_.  
So if we learn that _A_ is 'even', we become a little more confident
that _B_ is 'greater than three'. 

The mutual information $I(A;B)$ corresponds to the sum of the individual entropies,
minus the joint entropy:

\begin{aligned}
I(A;B) = H(A) + H(B) - H(A, B)
\end{aligned}



The entropy distance, also termed the variation of information,
is the information that _A_ and _B_ do _not_ have in common:

\begin{aligned}
H_D(A, B) = H(A, B) - I(A;B) = 2H(A,B) - H(A) - H(B)
\end{aligned}


```{r mackay-8-1, echo=FALSE, fig.width=4, out.width='50%', fig.height=3, fig.align='center'}
suppressMessages(library('TreeDist'))
H <- function(inBracket) {
  expression(paste(italic('H'), plain('('), italic(inBracket), plain(')')))
}
par(mar=c(3, 0.1, 0, 0))
joint <- Entropy(c(1, 2, 1, 2) / 6)
plot(0, type='n', xlim=c(0, joint), ylim=c(5, 0), axes=FALSE)
axis(1, at=c(0, 0.5, 1, 1.5, round(joint, 2)))
mtext('Entropy / bits', 1, 2)
rect(joint - 1, 3.1, 1, 3.9, col = Ternary::cbPalette8[3])
rect(0, 0.1, joint - 1, 0.9, col = Ternary::cbPalette8[5], border = NA)
rect(1, 1.1, joint, 1.9, col = Ternary::cbPalette8[5], border = NA)
rect(joint - 1, 1.1, 1, 1.9, col = Ternary::cbPalette8[3], border=NA)
rect(joint - 1, 0.1, 1, 0.9, col = Ternary::cbPalette8[3], border=NA)
text(1, 3.5, pos=4,
     expression(paste(italic(I), plain('('), italic('A;B'), plain(')'))))



rect(0, 2.1, joint, 2.9)
text(joint / 2, 2.5, 
     expression(paste(italic('H'), plain('('), italic('A, B'), plain(')'))))

rect(0, 0.1, 1, 0.9)
text(0.5, 0.5, 
     expression(paste(italic('H'), plain('('), italic(A), plain(')'))))


rect(joint - 1, 1.1, joint - 0, 1.9)
text(joint - 0.5, 1.5, 
     expression(paste(italic('H'), plain('('), italic(B), plain(')'))))


rect(0, 4.1, joint - 1, 4.9, col = Ternary::cbPalette8[5])
rect(1, 4.1, joint, 4.9,     col = Ternary::cbPalette8[5])
text(joint / 2, 4.5, 
     expression(paste(italic('H'['D']), plain('('), italic('A, B'), plain(')'))))


```

<!--
## Split entropy

Entropy is related to the probability of drawing a given tree
at random from a forest of permissable trees.



Let $X$ denote the 105 binary trees with six leaves, which are taken to be 
equally probable. 
The probability of drawing any particular tree $x$ from $X$ is
$P(x) = \frac{1}{105}$.
Because the ensemble $X$ is uniform, its entropy $H(X) =
\sum\nolimits_{x\in X}P(x)\log_2{\frac{1}{P(x)}} = \log_2{|X|}$.

The split $S_1 =$ `AB|CDEF` is found in fifteen 6-leaf trees; these trees form
a reduced ensemble with an entropy of $H(X|S_1) = \log_2{15} = 
3.91\textrm{ bits}$.
Likewise, the split $S_2 =$ `ABC|DEF` occurs in nine 6-leaf trees, so 
$H(X|S_2) = \log_2{9} = 3.17\textrm{ bits}$.

Three 6-leaf trees contain both $S_1$ and $S_2$; $H(X|S_1,S_2) = \log_2(3)
= 1.58\textrm{ bits}$.  

$H(X|S_1,S_2)$ is maximized when $S_1 = S_2$, but its value is highest when 
$S_1$ contains less phylogenetic information.  It is not clear that this is 
a useful quantity for measuring tree difference.-->

## References
