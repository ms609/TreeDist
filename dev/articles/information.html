<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Comparing splits using information theory • TreeDist</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Comparing splits using information theory">
<meta name="robots" content="noindex">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-BTKEG66D3F"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BTKEG66D3F');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">TreeDist</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">2.8.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home fa-lg"></span></a></li>
<li class="nav-item"><a class="nav-link" href="../articles/Using-TreeDist.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Function reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Tree distance analysis</h6></li>
    <li><a class="dropdown-item" href="../articles/Using-TreeDist.html">Calculate tree similarity with 'TreeDist'</a></li>
    <li><a class="dropdown-item" href="../articles/using-distances.html">Contextualizing tree distances</a></li>
    <li><a class="dropdown-item" href="../articles/different-leaves.html">Trees with different leaves</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Tree space analysis</h6></li>
    <li><a class="dropdown-item" href="../articles/treespace.html">Tree space analysis</a></li>
    <li><a class="dropdown-item" href="../articles/landscapes.html">Analysing landscapes of phylogenetic trees</a></li>
    <li><a class="dropdown-item" href="../articles/compare-treesets.html">Comparing sets of trees from different analyses</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Tree distance introductions</h6></li>
    <li><a class="dropdown-item" href="../articles/information.html">Comparing splits using information theory</a></li>
    <li><a class="dropdown-item" href="../articles/Robinson-Foulds.html">Extending the Robinson-Foulds metric</a></li>
    <li><a class="dropdown-item" href="../articles/Generalized-RF.html">Generalized Robinson-Foulds distances</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/ms609/TreeDist"><span class="fa fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Comparing splits using information theory</h1>
                        <h4 data-toc-skip class="author"><a href="https://smithlabdurham.github.io/" class="external-link">Martin R. Smith</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/ms609/TreeDist/blob/master/vignettes/information.Rmd" class="external-link"><code>vignettes/information.Rmd</code></a></small>
      <div class="d-none name"><code>information.Rmd</code></div>
    </div>

    
    
<p>To understand the <a href="Generalized-RF.html">information-based
metrics</a> implemented in ‘<a href="Using-TreeDist.html">TreeDist</a>’,
it is useful to recall some basic concepts of information theory.</p>
<p>For an introduction, see <span class="citation">MacKay (2003)</span>
or an introductory video to the clustering information distance:</p>
<p><a href="https://durham.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=ca5ede19-d21a-40ce-8b9e-ac6e00d7e2c0" class="external-link"><img src="CID_talk.png" alt="Introduction to the Clustering Info Distance"></a></p>
<div class="section level2">
<h2 id="splits">Splits<a class="anchor" aria-label="anchor" href="#splits"></a>
</h2>
<p>Each internal edge in a tree represents a split that divides its
leaves into two partitions. Intuitively, some splits are more
instructive than others. For example, the fact that mammals and reptiles
represent two separate groups is profound enough that it is worth
teaching to schoolchildren; much less information is represented by a
split that identifies two species of bat as more closely related to one
another than to any other mammal or reptile.</p>
</div>
<div class="section level2">
<h2 id="quantifying-information">Quantifying information<a class="anchor" aria-label="anchor" href="#quantifying-information"></a>
</h2>
<p>How can we formalize the intuition that some splits contain more
information than others? More generally, how can we quantify an amount
of information?</p>
<p>Information is usually measured in <em>bits</em>. One bit is the
amount of information generated by tossing a fair coin: to record the
outcome of a coin toss, I must record either a <code>H</code> or a
<code>T</code>, and with each of the two symbols equally likely, there
is no way to compress the results of multiple tosses.</p>
<p>The Shannon <span class="citation">(1948)</span> information content
of an outcome
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is defined to be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">h(x) = -\log_2{P(x)}</annotation></semantics></math>,
which simplifies to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mi>n</mi></mrow><annotation encoding="application/x-tex">\log_2{n}</annotation></semantics></math>
when all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
outcomes are equally likely. Thus, the outcome of a fair coin toss
delivers
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mn>2</mn><mo>=</mo><mn>1</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bit</mtext></mrow></mrow><annotation encoding="application/x-tex">\log_2{2} = 1\textrm{ bit}</annotation></semantics></math>
of information; the outcome of rolling a fair six-sided die contains
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mn>6</mn><mo>≈</mo><mn>2.58</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">\log_2{6} \approx 2.58\textrm{ bits}</annotation></semantics></math>
of information; and the outcome of selecting at random one of the 105
unrooted binary six-leaf trees is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mn>105</mn><mo>≈</mo><mn>6.71</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">\log_2{105} \approx 6.71\textrm{ bits}</annotation></semantics></math>.</p>
<p>Unlikely outcomes are more surprising, and thus contain more
information than likely outcomes. The information content of rolling a
twelve on two fair six-sided dice is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>1</mn><mn>36</mn></mfrac><mo>≈</mo><mn>5.16</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">-\log_2{\frac{1}{36}} \approx 5.16\textrm{ bits}</annotation></semantics></math>,
whereas a seven, which could be produced by six of the 36 possible rolls
(<code>1 &amp; 6</code>, <code>2 &amp; 5</code>, …), is less surprising,
and thus contains less information:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>6</mn><mn>36</mn></mfrac><mo>≈</mo><mn>2.58</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">-\log_2{\frac{6}{36}} \approx 2.58\textrm{ bits}</annotation></semantics></math>.
An additional 2.58 bits of information would be required to establish
which of the six possible rolls produced the seven.</p>
<div class="section level3">
<h3 id="application-to-splits">Application to splits<a class="anchor" aria-label="anchor" href="#application-to-splits"></a>
</h3>
<p>The split
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>=</mo></mrow><annotation encoding="application/x-tex">S_1 =</annotation></semantics></math><code>AB|CDEF</code> is found in 15 of the 105 six-leaf trees; as such,
the probability that a randomly drawn tree contains
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>15</mn><mn>105</mn></mfrac></mrow><annotation encoding="application/x-tex">P(S_1) = \frac{15}{105}</annotation></semantics></math>,
and the information content
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>15</mn><mn>105</mn></mfrac><mo>≈</mo><mn>2.81</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">h(S_1) =
-\log_2{\frac{15}{105}} \approx 2.81\textrm{ bits}</annotation></semantics></math>.
<span class="citation">Steel &amp; Penny (2006)</span> dub this quantity
the <strong>phylogenetic information content</strong>.</p>
<p>Likewise, the split
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>2</mn></msub><mo>=</mo></mrow><annotation encoding="application/x-tex">S_2 =</annotation></semantics></math><code>ABC|DEF</code> occurs in nine of the 105 six-leaf trees, so
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>9</mn><mn>105</mn></mfrac><mo>≈</mo><mn>3.54</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">h(S_2) = -\log_2{\frac{9}{105}} \approx 3.54\textrm{ bits}</annotation></semantics></math>.
Three six-leaf trees contain both splits, so in combination the splits
deliver
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>3</mn><mn>105</mn></mfrac><mo>≈</mo><mn>5.13</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">h(S_1,S_2) = -\log_2{\frac{3}{105}} \approx 5.13\textrm{ bits}</annotation></semantics></math>
of information.</p>
<p>Because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>&lt;</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(S_1,S_2) &lt; h(S_1) + h(S_2)</annotation></semantics></math>,
some of the information in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
is also present in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>.
The information in common between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>s</mi><mi>h</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>1.22</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">h_{shared}(S_1, S_2) =
h(S_1) + h(S_2) - h(S_1,S_2) \approx 1.22\textrm{ bits}</annotation></semantics></math>.
The information unique to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>d</mi><mi>i</mi><mi>f</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>3.91</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">h_{different}(S_1,S_2) =
2h(S_1,S_2) - h(S_1) - h(S_2) \approx 3.91\textrm{ bits}</annotation></semantics></math>.</p>
<p>These quantities can be calculated using functions in the ‘<a href="https://ms609.github.io/TreeTools/" class="external-link">TreeTools</a>’ package.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://ms609.github.io/TreeTools/" class="external-link">"TreeTools"</a></span>, quietly <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://ms609.github.io/TreeDist/">"TreeDist"</a></span><span class="op">)</span></span>
<span><span class="va">treesMatchingSplit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>  AB.CDEF <span class="op">=</span> <span class="fu"><a href="https://ms609.github.io/TreeTools/reference/TreesMatchingSplit.html" class="external-link">TreesMatchingSplit</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>  ABC.DEF <span class="op">=</span> <span class="fu"><a href="https://ms609.github.io/TreeTools/reference/TreesMatchingSplit.html" class="external-link">TreesMatchingSplit</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">treesMatchingSplit</span></span></code></pre></div>
<pre><code><span><span class="co">## AB.CDEF ABC.DEF </span></span>
<span><span class="co">##      15       9</span></span></code></pre>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">proportionMatchingSplit</span> <span class="op">&lt;-</span> <span class="va">treesMatchingSplit</span> <span class="op">/</span> <span class="fu"><a href="https://ms609.github.io/TreeTools/reference/NRooted.html" class="external-link">NUnrooted</a></span><span class="op">(</span><span class="fl">6</span><span class="op">)</span></span>
<span><span class="va">proportionMatchingSplit</span></span></code></pre></div>
<pre><code><span><span class="co">##    AB.CDEF    ABC.DEF </span></span>
<span><span class="co">## 0.14285714 0.08571429</span></span></code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">splitInformation</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log2</a></span><span class="op">(</span><span class="va">proportionMatchingSplit</span><span class="op">)</span></span>
<span><span class="va">splitInformation</span></span></code></pre></div>
<pre><code><span><span class="co">##  AB.CDEF  ABC.DEF </span></span>
<span><span class="co">## 2.807355 3.544321</span></span></code></pre>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">treesMatchingBoth</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/SplitSharedInformation.html">TreesConsistentWithTwoSplits</a></span><span class="op">(</span><span class="fl">6</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">combinedInformation</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log2</a></span><span class="op">(</span><span class="va">treesMatchingBoth</span> <span class="op">/</span> <span class="fu"><a href="https://ms609.github.io/TreeTools/reference/NRooted.html" class="external-link">NUnrooted</a></span><span class="op">(</span><span class="fl">6</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">sharedInformation</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">splitInformation</span><span class="op">)</span> <span class="op">-</span> <span class="va">combinedInformation</span></span>
<span><span class="va">sharedInformation</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1.222392</span></span></code></pre>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Or more concisely:</span></span>
<span><span class="fu"><a href="../reference/SplitSharedInformation.html">SplitSharedInformation</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">6</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1.222392</span></span></code></pre>
<!--The more similar the splits, the more information they will have in common;
the shared information is maximised when $S_1 = S_2$, and $h_{common} = h(S_1)$.
Likewise, more even splits contain more information than less even splits
(i.e. _h_(`AB|CDEF`) < _h_(`ABC|DEF`)).-->
</div>
</div>
<div class="section level2">
<h2 id="entropy">Entropy<a class="anchor" aria-label="anchor" href="#entropy"></a>
</h2>
<p>Entropy is the average information content of each outcome, weighted
by its probability:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∑</mo><mrow><mo>−</mo><mi>p</mi><msub><mo>log</mo><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sum{-p \log_2(p)}</annotation></semantics></math>.
Where all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
outcomes are equiprobable, this simplifies to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mi>n</mi></mrow><annotation encoding="application/x-tex">\log_2{n}</annotation></semantics></math>.</p>
<p>Consider a case in which Jane rolls a dice, and makes two true
statements about the outcome
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>:
“Is the roll even?”.</p>
<ul>
<li>Two equally-possible outcomes: yes or no</li>
<li>Entropy:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mo>log</mo><mn>2</mn></msub><mn>2</mn><mo>=</mo><mn>1</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bit</mtext></mrow></mrow><annotation encoding="application/x-tex">H(S_1) = \log_2{2} = 1\textrm{ bit}</annotation></semantics></math>.</li>
</ul>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>:
“Is the roll greater than 3?”</p>
<ul>
<li>Two equally-possible outcomes: yes or no</li>
<li>Entropy:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mo>log</mo><mn>2</mn></msub><mn>2</mn><mo>=</mo><mn>1</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bit</mtext></mrow></mrow><annotation encoding="application/x-tex">H(S_2) = \log_2{2} = 1\textrm{ bit}</annotation></semantics></math>.</li>
</ul>
<p>The joint entropy of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
is the entropy of the association matrix that considers each possible
outcome:</p>
<table class="table">
<colgroup>
<col width="25%">
<col width="41%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th> </th>
<th>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>:</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">S_1: x</annotation></semantics></math>
odd</th>
<th>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>:</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">S_1: x</annotation></semantics></math>
even</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>2</mn></msub><mo>:</mo><mi>x</mi><mo>≤</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">S_2: x \le 3</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mrow><mn>1</mn><mo>,</mo><mn>3</mn></mrow><mo>;</mo><mi>p</mi><mo>=</mo><mfrac><mn>2</mn><mn>6</mn></mfrac></mrow><annotation encoding="application/x-tex">x \in {1, 3}; p = \frac{2}{6}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>2</mn><mo>;</mo><mi>p</mi><mo>=</mo><mfrac><mn>1</mn><mn>6</mn></mfrac></mrow><annotation encoding="application/x-tex">x = 2; p = \frac{1}{6}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>2</mn></msub><mo>:</mo><mi>x</mi><mo>&gt;</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">S_2: x &gt; 3</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>5</mn><mo>;</mo><mi>p</mi><mo>=</mo><mfrac><mn>1</mn><mn>6</mn></mfrac></mrow><annotation encoding="application/x-tex">x = 5; p = \frac{1}{6}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mrow><mn>4</mn><mo>,</mo><mn>6</mn></mrow><mo>;</mo><mi>p</mi><mo>=</mo><mfrac><mn>2</mn><mn>6</mn></mfrac></mrow><annotation encoding="application/x-tex">x \in {4, 6}; p = \frac{2}{6}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>2</mn><mn>3</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>2</mn><mn>3</mn></mfrac><mo>+</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>1</mn><mn>3</mn></mfrac><mo>+</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>1</mn><mn>3</mn></mfrac><mo>+</mo><mfrac><mn>2</mn><mn>3</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>2</mn><mn>3</mn></mfrac><mo>≈</mo><mn>1.84</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
H(S_1, S_2) = \frac{2}{3}\log_2{\frac{2}{3}} + \frac{1}{3}\log_2{\frac{1}{3}} + \frac{1}{3}\log_2{\frac{1}{3}} + \frac{2}{3}\log_2{\frac{2}{3}} \approx
1.84 \textrm{ bits}
\end{aligned}</annotation></semantics></math></p>
<p>Note that this less than the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mn>2</mn></msub><mn>6</mn><mo>≈</mo><mn>2.58</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">\log_2{6} \approx 2.58\textrm{ bits}</annotation></semantics></math>
we require to determine the exact value of the roll: knowledge of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
is not guaranteed to be sufficient to unambiguously identify
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p>
<p>The mutual information between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
describes how much knowledge of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
reduces our uncertainty in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
(or <em>vice versa</em>). So if we learn that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
is ‘even’, we become a little more confident that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
is ‘greater than three’.</p>
<p>The mutual information
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>;</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(S_1;S_2)</annotation></semantics></math>,
denoted in blue below, corresponds to the sum of the individual
entropies, minus the joint entropy:</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>;</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
I(S_1;S_2) = H(S_1) + H(S_2) - H(S_1, S_2)
\end{aligned}</annotation></semantics></math><p>If two statements have high mutual information, then once you have
heard one statement, you already have a good idea what the outcome of
the other statement will be, and thus learn little new on hearing
it.</p>
<p>The entropy distance, also termed the variation of information <span class="citation">(Meila, 2007)</span>, corresponds to the information
that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
do <em>not</em> have in common (denoted below in yellow):</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>H</mi><mi>D</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>;</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
H_D(S_1, S_2) = H(S_1, S_2) - I(S_1;S_2) = 2H(S_1, S_2) - H(S_1) - H(S_2)
\end{aligned}</annotation></semantics></math><p>The higher the entropy distance, the harder it is to predict the
outcome of one statement from the other; the maximum entropy distance
occurs when the two statements are entirely independent.</p>
<p><img src="information_files/figure-html/mackay-8-1-1.png" width="50%" style="display: block; margin: auto;"></p>
<div class="section level3">
<h3 id="application-to-splits-1">Application to splits<a class="anchor" aria-label="anchor" href="#application-to-splits-1"></a>
</h3>
<p>A split divides leaves into two partitions. If we arbitrarily label
these partitions ‘A’ and ‘B’, and select a leaf at random, we can view
the partition label associated with the leaf. If 60/100 leaves belong to
partition ‘A’, and 40/100 to ‘B’, then the a leaf drawn at random has a
40% chance of bearing the label ‘A’; the split has an entropy of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mn>60</mn><mn>100</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>60</mn><mn>100</mn></mfrac><mo>−</mo><mfrac><mn>40</mn><mn>100</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>40</mn><mn>100</mn></mfrac><mo>≈</mo><mn>0.97</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow></mrow><annotation encoding="application/x-tex">-\frac{60}{100}\log_2{\frac{60}{100}}-\frac{40}{100}\log_2{\frac{40}{100}} \approx 0.97\textrm{ bits}</annotation></semantics></math>.</p>
<p>Now consider a different split, perhaps in a different tree, that
assigns 50 leaves from ‘A’ to a partition ‘C’, leaving the remaining 10
leaves from ‘A’, along with the 40 from ‘B’, in partition ‘D’. This
split has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mn>50</mn><mn>100</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>50</mn><mn>100</mn></mfrac><mo>−</mo><mfrac><mn>50</mn><mn>100</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>50</mn><mn>100</mn></mfrac><mo>=</mo><mn>1</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bit</mtext></mrow></mrow><annotation encoding="application/x-tex">-\frac{50}{100}\log_2{\frac{50}{100}}-\frac{50}{100}\log_2{\frac{50}{100}} = 1\textrm{ bit}</annotation></semantics></math>
of entropy.<br>
Put these together, and a randomly selected leaf may now bear one of
three possible labellings:</p>
<ul>
<li>‘A’ and ‘C’: 50 leaves</li>
<li>‘A’ and ‘D’: 10 leaves</li>
<li>‘B’ and ‘D’: 40 leaves.</li>
</ul>
<p>The two splits thus have a joint entropy of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mfrac><mn>50</mn><mn>100</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>50</mn><mn>100</mn></mfrac><mo>−</mo><mfrac><mn>10</mn><mn>100</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>10</mn><mn>100</mn></mfrac><mo>−</mo><mfrac><mn>40</mn><mn>100</mn></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mn>40</mn><mn>100</mn></mfrac><mo>≈</mo><mn>1.36</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> bits</mtext></mrow><mo>&lt;</mo><mn>0.97</mn><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-\frac{50}{100}\log_2{\frac{50}{100}}
-\frac{10}{100}\log_2{\frac{10}{100}}
-\frac{40}{100}\log_2{\frac{40}{100}} \approx 1.36\textrm{ bits} &lt; 0.97 + 1</annotation></semantics></math>.</p>
<p>The joint entropy is less than the sum of the individual entropies
because the two splits contain some mutual information: for instance, if
a leaf bears the label ‘B’, we can be certain that it will also bear the
label ‘D’. The more similar the splits are, and the more they agree in
their division of leaves, the more mutual information they will exhibit.
I term this the <strong>clustering information</strong>, in
contradistinction to the concept of phylogenetic information discussed
above.</p>
<p>More formally, let split
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
divides
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
leaves into two partitions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.
The probability that a randomly chosen leaf
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is in partition
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>∈</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><mi>k</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac></mrow><annotation encoding="application/x-tex">P(x \in k) = \frac{|k|}{n}</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
thus corresponds to a random variable with entropy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><mi>A</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><mi>A</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac><mo>−</mo><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac><msub><mo>log</mo><mn>2</mn></msub><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac></mrow><annotation encoding="application/x-tex">H(S) = -\frac{|A|}{n} \log_2{\frac{|A|}{n}} -
\frac{|B|}{n}\log_2{\frac{|B|}{n}}</annotation></semantics></math><span class="citation">(Meila, 2007)</span>. The joint entropy of two splits,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>,
corresponds to the entropy of the association matrix of probabilities
that a randomly selected leaf belongs to each pair of partitions:</p>
<table class="table">
<colgroup>
<col width="23%">
<col width="44%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th> </th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>:</mo><mi>x</mi><mo>∈</mo><msub><mi>A</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">S_1: x \in A_1</annotation></semantics></math></th>
<th><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>:</mo><mi>x</mi><mo>∈</mo><msub><mi>B</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">S_1: x \in B_1</annotation></semantics></math></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>2</mn></msub><mo>:</mo><mi>x</mi><mo>∈</mo><msub><mi>A</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">S_2: x \in A_2</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>A</mi><mn>1</mn></msub><mo>∩</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac></mrow><annotation encoding="application/x-tex">P(A_1,A_2) =
\frac{|A_1 \cap A_2|}{n}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>B</mi><mn>1</mn></msub><mo>∩</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac></mrow><annotation encoding="application/x-tex">P(B_1,A_2) = \frac{|B_1 \cap A_2|}{n}</annotation></semantics></math></td>
</tr>
<tr class="even">
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>2</mn></msub><mo>:</mo><mi>x</mi><mo>∈</mo><msub><mi>B</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">S_2: x \in B_2</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>B</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>A</mi><mn>1</mn></msub><mo>∩</mo><msub><mi>B</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac></mrow><annotation encoding="application/x-tex">P(A_1,B_2) =
\frac{|A_1 \cap B_2|}{n}</annotation></semantics></math></td>
<td><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><msub><mi>B</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>B</mi><mn>1</mn></msub><mo>∩</mo><msub><mi>B</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">|</mo></mrow><mi>n</mi></mfrac></mrow><annotation encoding="application/x-tex">P(B_1,B_2) = \frac{|B_1 \cap B_2|}{n}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mo>log</mo><mn>2</mn></msub><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mo>log</mo><mn>2</mn></msub><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">H(S_1, S_2) =
P(A_1,A_2) \log_2 {P(A_1,A_2)} +
P(B_1,A_2) \log_2 {P(B_1,A_2)}</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>B</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mo>log</mo><mn>2</mn></msub><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>B</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><msub><mi>B</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mo>log</mo><mn>2</mn></msub><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><msub><mi>B</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">+ P(A_1,B_2)\log_2{P(A_1,B_2)} +
P(B_1,B_2)\log_2{P(B_1,B_2)}</annotation></semantics></math></p>
<p>These values can then be substituted into the definitions of mutual
information and entropy distance given above.</p>
<p>As
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>
become more different, the disposition of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>1</mn></msub><annotation encoding="application/x-tex">S_1</annotation></semantics></math>
gives less information about the configuration of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mn>2</mn></msub><annotation encoding="application/x-tex">S_2</annotation></semantics></math>,
and the mutual information decreases accordingly.</p>
</div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-Mackay2003" class="csl-entry">
MacKay, D. J. C. (2003). <em>Information theory, inference, and learning
algorithms</em>. Cambridge: Cambridge University Press. Retrieved from
<a href="https://www.inference.org.uk/itprnn/book.pdf" class="external-link">https://www.inference.org.uk/itprnn/book.pdf</a>
</div>
<div id="ref-Meila2007" class="csl-entry">
Meila, M. (2007). <span class="nocase">Comparing clusterings—an
information based distance</span>. <em>Journal of Multivariate
Analysis</em>, <em>98</em>(5), 873–895. doi:<a href="https://doi.org/10.1016/j.jmva.2006.11.013" class="external-link">10.1016/j.jmva.2006.11.013</a>
</div>
<div id="ref-Shannon1948" class="csl-entry">
Shannon, C. E. (1948). A mathematical theory of communication. <em>Bell
System Technical Journal</em>, <em>27</em>, 379–423, 623–656.
</div>
<div id="ref-Steel2006" class="csl-entry">
Steel, M. A., &amp; Penny, D. (2006). Maximum parsimony and the
phylogenetic information in multistate characters. In V. A. Albert
(Ed.), <em>Parsimony, phylogeny, and genomics</em> (pp. 163–178).
Oxford: Oxford University Press.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://smithlabdurham.github.io/" class="external-link">Martin R. Smith</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
